1. Interview questions
    1. <a href="https://www.simplilearn.com/tutorials/deep-learning-tutorial/deep-learning-interview-questions
">deep-learning-interview-questions</a>
2. tenserflow interview questions
    1. <a href="https://www.javatpoint.com/tensorflow-interview-questions
">tensorflow-interview-questions</a>
3. Pytorch
    1. <a href="https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/
">pytorch-tutorial-develop-deep-learning-models</a>
    2. <a href="https://www.analyticsvidhya.com/blog/2019/09/introduction-to-pytorch-from-scratch/
">introduction-to-pytorch-from-scratch</a>
4. Why Relu Activation is better and theory of activation fucntions 3
    1. <a href="https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7 Best
">LINK</a>
    2. <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0
">understanding-activation-functions-in-neural-networks-9491262884e0</a>
    3. <a href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/
">rectified-linear-activation-function-for-deep-learning-neural-networks</a>
    4. <a href="https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/
">evolution-core-concepts-deep-learning-neural-networks</a>
    5. <a href="https://www.analyticsvidhya.com/blog/2017/05/25-must-know-terms-concepts-for-beginners-in-deep-learning/
">25-must-know-terms-concepts-for-beginners-in-deep-learning</a>
    6. <a href="https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/
">deep-learning-computer-vision-introduction-convolution-neural-networks</a>
5. Tensorflow
    1. <a href="https://www.analyticsvidhya.com/blog/2021/11/tensorflow-for-beginners-with-examples-and-python-implementation/
">tensorflow-for-beginners-with-examples-and-python-implementation</a>
    2. <a href="https://www.analyticsvidhya.com/blog/2016/10/an-introduction-to-implementing-neural-networks-using-tensorflow/?utm_source=blog&amp;utm_medium=comparison-deep-learning-framework
">LINK</a>
    3. <a href="https://www.analyticsvidhya.com/blog/2019/03/deep-learning-frameworks-comparison/
">deep-learning-frameworks-comparison</a>
    4. <a href="https://www.analyticsvidhya.com/blog/2016/08/deep-learning-path/
">deep-learning-path</a>
6. Optimizing Neural Network
    1. <a href="https://www.analyticsvidhya.com/blog/2016/10/tutorial-optimizing-neural-networks-using-keras-with-image-recognition-case-study/?utm_source=blog&amp;utm_medium=comparison-deep-learning-framework
">LINK</a>
    2. <a href="https://www.quora.com/Machine-Learning-What-are-some-tips-and-tricks-for-training-deep-neural-networks
">LINK</a>
7. Designing neural network
    1. <a href="https://towardsdatascience.com/designing-your-neural-networks-a5e4617027ed
">designing-your-neural-networks-a5e4617027ed</a>
8. Image Preprocessing 3
    1. <a href="https://theailearner.com/tag/image-preprocessing/
">image-preprocessing</a>
    2. <a href="https://docs.opencv.org/master/d4/d13/tutorial_py_filtering.html
">tutorial_py_filtering</a>
    3. <a href="https://www.tutorialkart.com/opencv/python/opencv-python-gaussian-image-smoothing/
">opencv-python-gaussian-image-smoothing</a>
    4. <a href="https://medium.com/@ariesiitr/image-processing-with-opencv-45c3a5cefd10
">image-processing-with-opencv-45c3a5cefd10</a>
9. https opencv python tutroals readthedocs io en latest py_tutorials py_imgproc py_thresholding py_thresholding html thresholding
    1. <a href="https://www.mygreatlearning.com/blog/introduction-to-image-pre-processing/
">introduction-to-image-pre-processing</a>
10. Exploding gradients in neural networks
    1. <a href="https://machinelearningmastery.com/exploding-gradients-in-neural-networks/
">exploding-gradients-in-neural-networks</a>
11. Initial layers Vs Last layers in CNN and increasing performance
    1. <a href="https://forums.fast.ai/t/why-do-initial-layers-learn-simple-patterns/31410
">31410</a>
    2. <a href="https://www.researchgate.net/post/Could_you_give_me_some_advices_about_how_to_improve_the_performance_of_Convolutional_Neural_Networks
">LINK</a>
12. Content based Image Retrieval CBIR using CNN Autoencoders
    1. <a href="https://medium.com/sicara/keras-tutorial-content-based-image-retrieval-convolutional-denoising-autoencoder-dc91450cc511
">keras-tutorial-content-based-image-retrieval-convolutional-denoising-autoencoder-dc91450cc511</a>
    2. <a href="https://www.pyimagesearch.com/2020/03/30/autoencoders-for-content-based-image-retrieval-with-keras-and-tensorflow/
">autoencoders-for-content-based-image-retrieval-with-keras-and-tensorflow</a>
    3. <a href="https://www.pyimagesearch.com/2020/02/17/autoencoders-with-keras-tensorflow-and-deep-learning/
">autoencoders-with-keras-tensorflow-and-deep-learning</a>
13. Content based Image Retrieval CBIR using using colour histogram
    1. <a href="https://www.pyimagesearch.com/2014/12/01/complete-guide-building-image-search-engine-python-opencv/
">complete-guide-building-image-search-engine-python-opencv</a>
    2. <a href="https://www.pyimagesearch.com/2014/01/22/clever-girl-a-guide-to-utilizing-color-histograms-for-computer-vision-and-image-search-engines/
">clever-girl-a-guide-to-utilizing-color-histograms-for-computer-vision-and-image-search-engines</a>
14. RNN
    1. <a href="https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce
">recurrent-neural-networks-d4642c9bc7ce</a>
    2. <a href="https://medium.com/explore-artificial-intelligence/an-introduction-to-recurrent-neural-networks-72c97bf0912
">an-introduction-to-recurrent-neural-networks-72c97bf0912</a>
    3. <a href="https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/
">fundamentals-of-deep-learning-introduction-to-lstm</a>
    4. <a href="https://www.analyticsvidhya.com/blog/2018/04/sequence-modelling-an-introduction-with-practical-use-cases/
">sequence-modelling-an-introduction-with-practical-use-cases</a>
    5. <a href="https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/
">introduction-to-recurrent-neural-networks</a>
    6. <a href="https://www.analyticsvidhya.com/blog/2019/01/fundamentals-deep-learning-recurrent-neural-networks-scratch-python/
">fundamentals-deep-learning-recurrent-neural-networks-scratch-python</a>
15. Pre Trained Deep learning models
16. https keras io api applications usage examples for image classification models
    1. <a href="https://towardsdatascience.com/the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96
">the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96</a>
    2. <a href="https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5
">cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5</a>
    3. <a href="https://www.analyticsvidhya.com/blog/2018/07/top-10-pretrained-models-get-started-deep-learning-part-1-computer-vision/
">top-10-pretrained-models-get-started-deep-learning-part-1-computer-vision</a>
    4. <a href="https://machinelearningmastery.com/how-to-use-transfer-learning-when-developing-convolutional-neural-network-models/
">how-to-use-transfer-learning-when-developing-convolutional-neural-network-models</a>
    5. <a href="https://medium.com/comet-ml/approach-pre-trained-deep-learning-models-with-caution-9f0ff739010c
">approach-pre-trained-deep-learning-models-with-caution-9f0ff739010c</a>
    6. <a href="https://www.tutorialspoint.com/keras/keras_pre_trained_models.htm
">keras_pre_trained_models</a>
17. DaTA Augmentation
    1. <a href="https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/
">how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks</a>
18. ealry stopping and checkpoiting
    1. <a href="https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/
">how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping</a>
19. Transfer Learning 3
    1. <a href="https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/
">transfer-learning-the-art-of-fine-tuning-a-pre-trained-model</a>
    2. <a href="https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751
">transfer-learning-from-pre-trained-models-f2393f124751</a>
20. sequence to sequence networks
    1. <a href="https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-1-processing-text-data-d141a5643b72
">nlp-sequence-to-sequence-networks-part-1-processing-text-data-d141a5643b72</a>
    2. <a href="https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-2-seq2seq-model-encoderdecoder-model-6c22e29fd7e1
">nlp-sequence-to-sequence-networks-part-2-seq2seq-model-encoderdecoder-model-6c22e29fd7e1</a>
    3. <a href="https://www.analyticsvidhya.com/blog/2018/03/essentials-of-deep-learning-sequence-to-sequence-modelling-with-attention-part-i/
">essentials-of-deep-learning-sequence-to-sequence-modelling-with-attention-part-i</a>
    4. <a href="https://www.analyticsvidhya.com/blog/2018/04/sequence-modelling-an-introduction-with-practical-use-cases/
">sequence-modelling-an-introduction-with-practical-use-cases</a>
21. DL good website
    1. <a href="https://www.wandb.com/
">www</a>
22. Keras Gridsearch
23. http queirozf com entries scikit learn pipeline examples pipeline with a keras model
    1. <a href="https://stackoverflow.com/questions/47527915/keras-scikit-learn-wrapper-appears-to-hang-when-gridsearchcv-with-n-jobs-1
">keras-scikit-learn-wrapper-appears-to-hang-when-gridsearchcv-with-n-jobs-1</a>
    2. <a href="https://blogs.oracle.com/meena/simple-neural-network-model-using-keras-and-grid-search-hyperparameterstuning
">simple-neural-network-model-using-keras-and-grid-search-hyperparameterstuning</a>
    3. <a href="https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/
">grid-search-hyperparameters-deep-learning-models-python-keras</a>
24. One hot incoding for different set of categories in train and test data
    1. <a href="https://medium.com/vickdata/easier-machine-learning-with-the-new-column-transformer-from-scikit-learn-c2268ea9564c
">easier-machine-learning-with-the-new-column-transformer-from-scikit-learn-c2268ea9564c</a>
25. PCA
    1. <a href="https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60
">pca-using-python-scikit-learn-e653f8989e60</a>
26. Deploying Tenserflow model
    1. <a href="https://towardsdatascience.com/deploy-tensorflow-models-9813b5a705d5
">deploy-tensorflow-models-9813b5a705d5</a>
    2. <a href="https://www.freecodecamp.org/news/how-to-deploy-tensorflow-models-to-production-using-tf-serving-4b4b78d41700/ 
">LINK</a>
27. pyimagesearch
    1. <a href="https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/
">keras-conv2d-and-convolutional-layers</a>
    2. <a href="https://www.pyimagesearch.com/2018/05/07/multi-label-classification-with-keras/
">multi-label-classification-with-keras</a>
    3. <a href="https://www.pyimagesearch.com/2017/12/11/image-classification-with-keras-and-deep-learning/
">image-classification-with-keras-and-deep-learning</a>
    4. <a href="https://www.pyimagesearch.com/2018/09/10/keras-tutorial-how-to-get-started-with-keras-deep-learning-and-python/
">keras-tutorial-how-to-get-started-with-keras-deep-learning-and-python</a>
28. for rest api
    1. <a href="https://www.pyimagesearch.com/2018/02/05/deep-learning-production-keras-redis-flask-apache/
">deep-learning-production-keras-redis-flask-apache</a>
    2. <a href="https://www.pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/
">scalable-keras-deep-learning-rest-api</a>
 
KEY POINTS
1. The biggest downside to VGG16 (besides how long it takes to train) is the resulting model
2. size, weighing in at over 533MB. If you were building a deep learning application and intended
3. to ship the model size with your app, you would already have a package > 500MB to distribute.
4. Furthermore, all software is updated at some point, requiring you to repackage and re-distribute a
5. large 500MB package, likely over a network connection. For high-speed broadband connections,
6. this large model size may not be an issue. But for resource constrained devices such as embedded
7. devices, cell phones, and even self-driving cars, this 500MB model size can be a huge burden. In
8. these types of situations, we prefer very small model sizes.
9. Luckily, all remaining models we’ll discuss in this bundle are substantially smaller than
10. VGGNet. The weights for our highly accurate ResNet model come in at 102MB. GoogLeNet is
11. even smaller at 28MB. And the super tiny, efficient SqueezeNet model size is only 4.9MB, making
12. it ideal for any type of resource constrained deep learning.
13. http://contrib.scikit-learn.org/categorical-encoding/index.html
14. In 2013 a variant of dropout is introduced called Drop connect. Instead of dropping hidden units with certain probability weights are randomly dropped with certain probability. It has been shown that a Drop connect network seemed to perform better than dropout on the MNIST data set.
15. difference between Vanila Gradient and SGD optimiser: Vanilla gradient descent only performs a weight update once for every epoch – in this example, we
16. trained our model for 100 epochs, so only 100 updates took place. Depending on the initialisation of the weight matrix and the size of the learning rate, it’s possible that we may not be able to learn a model that can separate the points (even though they are linearly separable).
17. For simple gradient descent, you are better off training for more epochs with a smaller learning rate to help overcome this issue. However, as we’ll see in the next section, a variant of gradient descent called Stochastic Gradient Descent performs a weight update for every batch of training data, implying there are multiple weight updates per epoch. This approach leads to a faster, more stable convergence.
18. backpropogation:The backward pass where we compute the gradient of the loss function at the final layer (i.e.,
19. predictions layer) of the network and use this gradient to recursively apply the chain rule to
20. update the weights in our network (also known as the weight update phase).
21. Backpropagation Summary:
22. In this section, we learned how to implement the backpropagation algorithm from scratch using
23. Python. Backpropagation is a generalization of the gradient descent family of algorithms that is
24. specifically used to train multi-layer feedforward networks.
25. The backpropagation algorithm consists of two phases:
26. 1. The forward pass where we pass our inputs through the network to obtain our output
27. classifications.
28. 10.1 Neural Network Basics 153
29. 2. The backward pass (i.e., weight update phase) where we compute the gradient of the loss
30. function and use this information to iteratively apply the chain rule to update the weights in
31. our network.
32. Regardless of whether we are working with simple feedforward neural networks or complex,
33. deep Convolutional Neural Networks, the backpropagation algorithm is still used to train these
34. models. This is accomplished by ensuring that the activation functions inside the network are
35. differentiable, allowing the chain rule to be applied. Furthermore, any other layers inside the network
36. that require updates to their weights/parameters, must also be compatible with backpropagation as
37. well.
38. CNN : In traditional feedforward neural networks (like the ones we studied in Chapter 10), each
39. neuron in the input layer is connected to every output neuron in the next layer – we call this a fullyconnected
40. (FC) layer. However, in CNNs, we don’t use FC layers until the very last layer(s) in the
41. network. We can thus define a CNN as a neural network that swaps in a specialized “convolutional”
42. layer in place of “fully-connected” layer for at least one of the layers in the network [10].
43. A nonlinear activation function, such as ReLU, is then applied to the output of these convolutions
44. and the process of convolution => activation continues (along with a mixture of other layer types to
45. help reduce the width and height of the input volume and help reduce overfitting) until we finally
46. reach the end of the network and apply one or two FC layers where we can obtain our final output
47. classifications.
48. Each layer in a CNN applies a different set of filters, typically hundreds or thousands of them,
49. and combines the results, feeding the output into the next layer in the network. During training, a
50. CNN automatically learns the values for these filters.
51. In the context of image classification, our CNN may learn to:
52. Detect edges from raw pixel data in the first layer.
53. Use these edges to detect shapes (i.e., “blobs”) in the second layer.
54. Use these shapes to detect higher-level features such as facial structures, parts of a car, etc.
55. in the highest layers of the network.
56. The last layer in a CNN uses these higher-level features to make predictions regarding the
57. contents of the image. In practice, CNNs give us two key benefits: local invariance and compositionality.
58. The concept of local invariance allows us to classify an image as containing a particular
59. object regardless of where in the image the object appears. We obtain this local invariance through
60. the usage of "pooling layers” (discussed later in this chapter) which identifies regions of our input
61. volume with a high response to a particular filter.
62. The second benefit is compositionality. Each filter composes a local patch of lower-level features
63. into a higher-level representation, similar to how we can compose a set of mathematical functions
64. that build on the output of previous functions: f (g(h(x))) – this composition allows our network to
65. learn more rich features deeper in the network. For example, our network may build edges from
66. pixels, shapes from edges, and then complex objects from shapes – all in an automated fashion that
67. happens naturally during the training process. The concept of building higher-level features from
68. lower-level ones is exactly why CNNs are so powerful in computer vision.
69. It’s normal to hand-define kernels to obtain various image processing functions. In fact, you
70. might already be familiar with blurring (average smoothing, Gaussian smoothing, etc.), edge
71. detection (Laplacian, Sobel, Scharr, Prewitt, etc.), and sharpening – all of these operations are
72. forms of hand-defined kernels that are specifically designed to perform a particular function.
73. Layer Types
74. There are many types of layers used to build Convolutional Neural Networks, but the ones you are
75. most likely to encounter include:
76. Convolutional (CONV)
77. Activation (ACT or RELU, where we use the same or the actual activation function)
78. Pooling (POOL)
79. Fully-connected (FC)
80. Batch normalization (BN)
81. Dropout (DO)
82. Stacking a series of these layers in a specific manner yields a CNN. We often use simple text
83. diagrams to describe a CNN: INPUT => CONV => RELU => FC => SOFTMAX
84. Here we define a simple CNN that accepts an input, applies a convolution layer, then an
85. activation layer, then a fully-connected layer, and, finally, a softmax classifier to obtain the output
86. classification probabilities. The SOFTMAX activation layer is often omitted from the network diagram
87. as it is assumed it directly follows the final FC.
88. Of these layer types, CONV and FC, (and to a lesser extent, BN) are the only layers that contain
89. parameters that are learned during the training process. Activation and dropout layers are
90. not considered true “layers" themselves, but are often included in network diagrams to make the
91. architecture explicitly clear. Pooling layers (POOL), of equal importance as CONV and FC, are also
92. included in network diagrams as they have a substantial impact on the spatial dimensions of an
93. image as it moves through a CNN.
94. CONV, POOL, RELU, and FC are the most important when defining your actual network
95. architecture. That’s not to say that the other layers are not critical, but take a backseat to this
96. critical set of four as they define the actual architecture itself.
97. We then have a kernel responsible for sharpening an image:
98. 55 construct a sharpening filter
99. 56 sharpen = np.array((
100. 57 [0, -1, 0],
101. 58 [-1, 5, -1],
102. 59 [0, -1, 0]), dtype="int")
103. Then the Laplacian kernel used to detect edge-like regions:
104. 11.1 Understanding Convolutions 177
105. 61 construct the Laplacian kernel used to detect edge-like
106. 62 regions of an image
107. 63 laplacian = np.array((
108. 64 [0, 1, 0],
109. 65 [1, -4, 1],
110. 66 [0, 1, 0]), dtype="int")
111. The Sobel kernels can be used to detect edge-like regions along both the x and y axis, respectively:
112. 68 construct the Sobel x-axis kernel
113. 69 sobelX = np.array((
114. 70 [-1, 0, 1],
115. 71 [-2, 0, 2],
116. 72 [-1, 0, 1]), dtype="int")
117. 74 construct the Sobel y-axis kernel
118. 75 sobelY = np.array((
119. 76 [-1, -2, -1],
120. 77 [0, 0, 0],
121. 78 [1, 2, 1]), dtype="int")
122. And finally, we define the emboss kernel:
123. 80 construct an emboss kernel
124. 81 emboss = np.array((
125. 82 [-2, -1, 0],
126. 83 [-1, 1, 1],
127. 84 [0, 1, 2]), dtype="int")
128. Explaining how each of these kernels were formulated is outside the scope of this book, so for
129. the time being simply understand that these are kernels that were manually built to perform a given
130. operation.
131. For a thorough treatment of how kernels are mathematically constructed and proven to perform
132. a given image processing operation, please refer to Szeliksi (Chapter 3) [124]. I also recommend
133. using this excellent kernel visualization tool from Setosa.io [125].
134. Given all these kernels, we can lump them together into a set of tuples called a “kernel bank”:
135. 86 construct the kernel bank, a list of kernels we’re going to apply
136. 87 using both our custom ‘convolve‘ function and OpenCV’s ‘filter2D‘
137. 88 function
138. 89 kernelBank = (
139. 90 ("small_blur", smallBlur),
140. 91 ("large_blur", largeBlur),
141. 92 ("sharpen", sharpen),
142. 93 ("laplacian", laplacian),
143. 94 ("sobel_x", sobelX),
144. 95 ("sobel_y", sobelY),
145. 96 ("emboss", emboss))
146. Zero-padding: As we know from Section 11.1.5, we need to “pad” the borders of an image to retain the original
147. image size when applying a convolution – the same is true for filters inside of a CNN. Using
148. zero-padding, we can “pad” our input along the borders such that our output volume size matches
149. our input volume size. The amount of padding we apply is controlled by the parameter P.
150. This technique is especially critical when we start looking at deep CNN architectures that apply
151. multiple CONV filters on top of each other. To visualize zero-padding, again refer to Table 11.1
152. where we applied a 33 Laplacian kernel to a 55 input image with a stride of S = 1.
153. We can see in Table 11.3 (left) how the output volume is smaller (33) than the input volume
154. (55) due to the nature of the convolution operation. If we instead set P = 1, we can pad our
155. input volume with zeros (right) to create a 77 volume and then apply the convolution operation,
156. leading to an output volume size that matches the original input volume size of 55 (bottom).
157. Without zero padding, the spatial dimensions of the input volume would decrease too quickly,
158. and we wouldn’t be able to train deep networks (as the input volumes would be too tiny to learn
159. any useful patterns from).
160. Putting all these parameters together, we can compute the size of an output volume as a function
161. of the input volume size (W, assuming the input images are square, which they nearly always are),
162. the receptive field size F, the stride S, and the amount of zero-padding P. To construct a valid CONV
163. layer, we need to ensure the following equation is an integer:
164. ((W-F +2P)=S)+1
165. If it is not an integer, then the strides are set incorrectly, and the neurons cannot be tiled such
166. that they fit across the input volume in a symmetric way.
167. Pooling Layers: There are two methods to reduce the size of an input volume – CONV layers with a stride > 1 (which
168. we’ve already seen) and POOL layers. It is common to insert POOL layers in-between consecutive
169. CONV layers in a CNN architectures:
170. INPUT => CONV => RELU => POOL => CONV => RELU => POOL => FC
171. The primary function of the POOL layer is to progressively reduce the spatial size (i.e., width
172. and height) of the input volume. Doing this allows us to reduce the amount of parameters and
173. computation in the network – pooling also helps us control overfitting.
174. POOL layers operate on each of the depth slices of an input independently using either the
175. max or average function. Max pooling is typically done in the middle of the CNN architecture
176. to reduce spatial size, whereas average pooling is normally used as the final layer of the network
177. (e.x., GoogLeNet, SqueezeNet, ResNet) where we wish to avoid using FC layers entirely. The most
178. common type of POOL layer is max pooling, although this trend is changing with the introduction
179. of more exotic micro-architectures.
180. To POOL or CONV?
181. In their 2014 paper, Striving for Simplicity: The All Convolutional Net, Springenberg et al. [127]
182. recommend discarding the POOL layer entirely and simply relying on CONV layers with a larger
183. stride to handle downsampling the spatial dimensions of the volume. Their work demonstrated this
184. approach works very well on a variety of datasets, including CIFAR-10 (small images, low number
185. of classes) and ImageNet (large input images, 1,000 classes). This trend continues with the ResNet
186. architecture [101] which uses CONV layers for downsampling as well.
187. It’s becoming increasingly more common to not use POOL layers in the middle of the network
188. architecture and only use average pooling at the end of the network if FC layers are to be avoided.
189. Perhaps in the future there won’t be pooling layers in Convolutional Neural Networks – but in the
190. meantime, it’s important that we study them, learn how they work, and apply them to our own
191. architectures.
192. CNN Convulation Layer:
193. The CONV layer is the core building block of a Convolutional Neural Network. The CONV layer
194. parameters consist of a set of K learnable filters (i.e., “kernels”), where each filter has a width and a
195. height, and are nearly always square. These filters are small (in terms of their spatial dimensions)
196. but extend throughout the full depth of the volume
197. For inputs to the CNN, the depth is the number of channels in the image (i.e., a depth of three
198. when working with RGB images, one for each channel). For volumes deeper in the network, the
199. depth will be the number of filters applied in the previous layer.
200. To make this concept more clear, let’s consider the forward-pass of a CNN, where we convolve
201. each of the K filters across the width and height of the input volume, just like we did in Section
202. 11.1.5 above. More simply, we can think of each of our K kernels sliding across the input region,
203. computing an element-wise multiplication, summing, and then storing the output value in a 2-
204. dimensional activation map, such as in Figure 11.6.
205. After applying all K filters to the input volume, we now have K, 2-dimensional activation maps.
206. We then stack our K activation maps along the depth dimension of our array to form the final output
207. volume (Figure 11.7).
208. Every entry in the output volume is thus an output of a neuron that “looks” at only a small
209. region of the input. In this manner, the network “learns” filters that activate when they see a specific
210. type of feature at a given spatial location in the input volume. In lower layers of the network, filters
211. may activate when they see edge-like or corner-like regions.
212. Then, in the deeper layers of the network, filters may activate in the presence of high-level
213. features, such as parts of the face, the paw of a dog, the hood of a car, etc. This activation concept
214. goes back to our neural network analogy in Chapter 10 – these neurons are becoming “excited” and
215. “activating” when they see a particular pattern in an input image.
216. why earch neuron in current layer is not connected to every neoron in previous layer in CNN:
217. The concept of convolving a small filter with a large(r) input volume has special meaning in
218. Convolutional Neural Networks – specifically, the local connectivity and the receptive field of
219. a neuron. When working with images, it’s often impractical to connect neurons in the current
220. volume to all neurons in the previous volume – there are simply too many connections and too
221. many weights, making it impossible to train deep networks on images with large spatial dimensions.
222. Instead, when utilizing CNNs, we choose to connect each neuron to only a local region of the input
223. volume – we call the size of this local region the receptive field (or simply, the variable F) of the
224. neuron.
225. To make this point clear, let’s return to our CIFAR-10 dataset where the input volume has an
226. input size of 32323. Each image thus has a width of 32 pixels, a height of 32 pixels, and a
227. depth of 3 (one for each RGB channel). If our receptive field is of size 33, then each neuron in
228. the CONV layer will connect to a 33 local region of the image for a total of 333 = 27 weights
229. (remember, the depth of the filters is three because they extend through the full depth of the input
230. image, in this case, three channels).
231. Batch Normalization
232. Batch normalization has been shown to be extremely effective at reducing the number of
233. epochs it takes to train a neural network. Batch normalization also has the added benefit of helping
234. “stabilize” training, allowing for a larger variety of learning rates and regularization strengths. Using
235. batch normalization doesn’t alleviate the need to tune these parameters of course, but it will make
236. your life easier by making learning rate and regularization less volatile and more straightforward
237. to tune. You’ll also tend to notice lower final loss and a more stable loss curve when using batch
238. normalization in your networks.
239. The biggest drawback of batch normalization is that it can actually slow down the wall time it
240. takes to train your network (even though you’ll need fewer epochs to obtain reasonable accuracy)
241. by 2-3x due to the computation of per-batch statistics and normalization.
242. I can confirm that in nearly all experiments I’ve performed with CNNs, placing the BN after the
243. RELU yields slightly higher accuracy and lower loss.
244. most common CNN layer architecture:
245. INPUT => [[CONV => RELU]*N => POOL?]*M => [FC => RELU]*K => FC
246. Alexnet Architecture
247. INPUT => [CONV => RELU => POOL] * 2 => [CONV => RELU] * 3 => POOL =>[FC => RELU => DO] * 2 => SOFTMAX
248. VGGNET Architecture
249. For deeper network architectures, such as VGGNet [100], we’ll stack two (or more) layers
250. before every POOL layer:
251. INPUT => [CONV => RELU] * 2 => POOL => [CONV => RELU] * 2 => POOL =>[CONV => RELU] * 3 => POOL => [CONV => RELU] * 3 => POOL =>[FC => RELU => DO] * 2 => SOFTMAX
252. Generally, we apply deeper network architectures when we (1) have lots of labeled training
253. data and (2) the classification problem is sufficiently challenging. Stacking multiple CONV layers
254. before applying a POOL layer allows the CONV layers to develop more complex features before the
255. destructive pooling operation is performed.
256. Somearchitectures remove the POOL operation entirely, relying on CONV layers to downsample the volume
257. – then, at the end of the network, average pooling is applied rather than FC layers to obtain the input
258. to the softmax classifiers.
259. Network architectures such as GoogLeNet, ResNet, and SqueezeNet [101, 102, 132] are great
260. examples of this pattern and demonstrate how removing FC layers leads to less parameters and
261. faster training time.
262. Rules of Thumbs in CNN:
263. In this section, I’ll review common rules of thumb when constructing your own CNNs. To start,
264. the images presented to the input layer should be square. Using square inputs allows us to take
265. advantage of linear algebra optimization libraries. Common input layer sizes include 3232,
266. 11.3 Common Architectures and Training Patterns 193
267. 6464, 9696, 224224, 227227 and 229229 (leaving out the number of channels for
268. notational convenience).
269. Secondly, the input layer should also be divisible by two multiple times after the first CONV
270. operation is applied. You can do this by tweaking your filter size and stride. The “divisible by two
271. rule" enables the spatial inputs in our network to be conveniently down sampled via POOL operation
272. in an efficient manner.
273. In general, your CONV layers should use smaller filter sizes such as 33 and 55. Tiny
274. 11 filters are used to learn local features, but only in your more advanced network architectures.
275. Larger filter sizes such as 77 and 1111 may be used as the first CONV layer in the network (to
276. reduce spatial input size, provided your images are sufficiently larger than > 200200 pixels);
277. however, after this initial CONV layer the filter size should drop dramatically, otherwise you will
278. reduce the spatial dimensions of your volume too quickly.
279. You’ll also commonly use a stride of S = 1 for CONV layers, at least for smaller spatial input
280. volumes (networks that accept larger input volumes use a stride S >= 2 in the first CONV layer to
281. help reduce spatial dimensions). Using a stride of S = 1 enables our CONV layers to learn filters
282. while the POOL layer is responsible for downsampling. However, keep in mind that not all network
283. architectures follow this pattern – some architectures skip max pooling altogether and rely on the
284. CONV stride to reduce volume size.
285. My personal preference is to apply zero-padding to my CONV layers to ensure the output
286. dimension size matches the input dimension size – the only exception to this rule is if I want
287. to purposely reduce spatial dimensions via convolution. Applying zero-padding when stacking
288. multiple CONV layers on top of each other has also demonstrated to increase classification accuracy
289. in practice. As we’ll see later in this book, libraries such as Keras can automatically compute
290. zero-padding for you, making it even easier to build CNN architectures.
291. A second personal recommendation is to use POOL layers (rather than CONV layers) to reduce
292. the spatial dimensions of your input, at least until you become more experienced constructing your
293. own CNN architectures. Once you reach that point, you should start experimenting with using CONV
294. layers to reduce spatial input size and try removing max pooling layers from your architecture.
295. Most commonly, you’ll see max pooling applied over a 22 receptive field size and a stride of
296. S = 2. You might also see a 33 receptive field early in the network architecture to help reduce
297. image size. It is highly uncommon to see receptive fields larger than three since these operations
298. are very destructive to their inputs.
299. Batch normalization is an expensive operation which can double or triple the amount of time it
300. takes to train your CNN; however, I recommend using BN in nearly all situations. While BN does
301. indeed slow down the training time, it also tends to “stabilize” training, making it easier to tune
302. other hyperparameters (there are some exceptions, of course – I detail a few of these “exception
303. architectures" inside the ImageNet Bundle).
304. I also place the batch normalization after the activation, as has become commonplace in the
305. deep learning community even though it goes against the original Ioffe and Szegedy paper [128].
306. Inserting BN into the common layer architectures above, they become:
307.  INPUT => CONV => RELU => BN => FC
308.  INPUT => [CONV => RELU => BN => POOL] * 2 => FC => RELU => BN => FC
309.  INPUT => [CONV => RELU => BN => CONV => RELU => BN => POOL] * 3 => [FC =>
310. RELU => BN] * 2 => FC
311. You do not apply batch normalization before the softmax classifier as at this point we assume
312. our network has learned its discriminative features earlier in the architecture.
313. Dropout (DO) is typically applied in between FC layers with a dropout probability of 50% –
314. you should consider applying dropout in nearly every architecture you build. While not always
315. performed, I also like to include dropout layers (with a very small probability, 10-25%) between
316. 194 Chapter 11. Convolutional Neural Networks
317. POOL and CONV layers. Due to the local connectivity of CONV layers, dropout is less effective here,
318. but I’ve often found it helpful when battling overfitting.
319. By keeping these rules of thumb in mind, you’ll be able to reduce your headaches when
320. constructing CNN architectures since your CONV layers will preserve input sizes while the POOL
321. layers take care of reducing spatial dimensions of the volumes, eventually leading to FC layers and
322. the final output classifications.
323. Once you master this “traditional” method of building Convolutional Neural Networks, you
324. should then start exploring leaving max pooling operations out entirely and using just CONV layers
325. to reduce spatial dimensions, eventually leading to average pooling rather than an FC layer – these
326. types of more advanced architecture techniques are covered inside the ImageNet Bundle.
327. The first layer of the CNN usually includes filter sizes somewhere between 77 [99] and
328. 1111 [133]. From there, filter sizes progressively reduced to 55. Finally, only the deepest
329. layers of the network used 33 filters.
330. VGGNet is unique in that it uses 33 kernels throughout the entire architecture. The use
331. of these small kernels is arguably what helps VGGNet generalize to classification problems outside
332. what the network was originally trained on (we’ll see this inside the Practitioner Bundle and
333. ImageNet Bundle when we discuss transfer learning).
334. Any time you see a network architecture that consists entirely of 33 filters, you can rest
335. assured that it was inspired by VGGNet. Reviewing the entire 16 and 19 layer variants of VGGNet
336. is too advanced for this introduction to Convolutional Neural Networks – for
337. The VGG Family of Networks
338. The VGG family of Convolutional Neural Networks can be characterized by two key components:
339. 1. All CONV layers in the network using only 33 filters.
340. 2. Stacking multiple CONV => RELU layer sets (where the number of consecutive CONV =>
341. RELU layers normally increases the deeper we go) before applying a POOL operation
342. Data Augmentation
343. Keras Class NAme-ImageDataGenerator:
344. aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,
345. 28 height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,
346. 29 horizontal_flip=True, fill_mode="nearest")
347. Notice how each image has been randomly rotated, sheared, zoomed, and horizontally
348. flipped. In each case the image retains the original class label: dog; however, each image has been
349. modified slightly, thereby giving our neural network new patterns to learn from when training. Since
350. the input images will constantly be changing (while the class labels remain the same), it’s common
351. to see our training accuracy decrease when compared to training without data augmentation.
352. However, as we’ll find out later in this chapter, data augmentation can help dramatically reduce
353. overfitting, all the while ensuring that our model generalizes better to new input samples. Furthermore,
354. when working with datasets where we have too few examples to apply deep learning, we
355. can utilize data augmentation to generate additional training data, thereby reducing the amount of
356. hand-labeled data required to train a deep learning network.
357. resizing images while maintaining aspect ratio
358. However, for more challenging datasets we should still seek to resize to a fixed size, but
359. maintain the aspect ratio. To visualize this action, consider Figure 2.4.
360. On the left, we have an input image that we need to resize to a fixed width and height. Ignoring
361. the aspect ratio, we resize the image to 256256 pixels (middle), effectively squishing and
362. distorting the image such that it meets our desired dimensions. A better approach would be to
363. take into account the aspect ratio of the image (right) where we first resize along the shorter
364. dimension such that the width is 256 pixels and then crop the image along the height, such that the
365. height is 256 pixels.
366. While we have effectively discarded part of the image during the crop, we have also maintained
367. the original aspect ratio of the image. Maintaining a consistent aspect ratio allows our Convolutional
368. Neural Network to learn more discriminative, consistent features.
369. Data augmentation is a type of regularisation technique that operates on the training data. As the
370. name suggests, data augmentation randomly jitters our training data by applying a series of random
371. translations, rotations, shears, and flips. Applying these simple transformations does not change the
372. class label of the input image; however, each augmented image can be considered a “new” image
373. that the training algorithm has not seen before. Therefore, our training algorithm is being constantly
374. presented with new training samples, allowing it to learn more robust and discriminative patterns.
375. When it comes to your own experiments, you should apply data augmentation to nearly every experiment you run. There is a slight performance
376. hit you must take due to the fact that the CPU is now responsible for randomly transforming your
377. inputs; however, this performance hit is mitigated by using threading and augmenting your data in
378. the background before it is passed to the thread responsible for training your network.
