1. Encoding techniques
    1. <a href="https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/
">types-of-categorical-data-encoding</a>
2. difference between normalization and standardization
    1. <a href="https://www.geeksforgeeks.org/normalization-vs-standardization/
">normalization-vs-standardization</a>
    2. <a href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/
">feature-scaling-machine-learning-normalization-standardization</a>
3. Boosting
    1. <a href="https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/
">an-end-to-end-guide-to-understand-the-math-behind-xgboost</a>
    2. <a href="https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/
">complete-guide-parameter-tuning-gradient-boosting-gbm-python</a>
    3. <a href="https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/
">comprehensive-guide-for-ensemble-models</a>
4. GBM Internal working
    1. <a href="https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d
">gradient-boosting-from-scratch-1e317ae4587d</a>
    2. <a href="https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab
">understanding-gradient-boosting-machines-9be756fe76ab</a>
5. XGboost and Light GBM difference
    1. <a href="https://neptune.ai/blog/xgboost-vs-lightgbm
">xgboost-vs-lightgbm</a>
6. stacking and blending difference
    1. <a href="https://neptune.ai/blog/ensemble-learning-guide
">ensemble-learning-guide</a>
7. Analytics vidya links
    1. <a href="https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/
">guide-data-exploration</a>
    2. <a href="https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/
">12-pandas-techniques-python-data-manipulation</a>
8. pyspark
    1. <a href="https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/?utm_source=blog&utm_medium=streaming-data-pyspark-machine-learning-model
">LINK</a>
    2. <a href="https://www.analyticsvidhya.com/blog/2019/12/streaming-data-pyspark-machine-learning-model/
">streaming-data-pyspark-machine-learning-model</a>
9. Linear Regression
    1. <a href="https://www.analyticsvidhya.com/blog/2017/07/30-questions-to-test-a-data-scientist-on-linear-regression/
">30-questions-to-test-a-data-scientist-on-linear-regression</a>
    2. <a href="https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
">comprehensive-guide-regression</a>
    3. <a href="https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/
">a-comprehensive-guide-for-linear-ridge-and-lasso-regression</a>
10. Statistics
    1. <a href="https://www.analyticsvidhya.com/blog/2017/01/comprehensive-practical-guide-inferential-statistics-data-science/
">comprehensive-practical-guide-inferential-statistics-data-science</a>
    2. <a href="https://www.analyticsvidhya.com/blog/2020/07/hypothesis-testing-68351/
">hypothesis-testing-68351</a>
    3. <a href="https://www.analyticsvidhya.com/blog/2020/06/statistics-analytics-hypothesis-testing-z-test-t-test/
">statistics-analytics-hypothesis-testing-z-test-t-test</a>
    4. <a href="https://www.analyticsvidhya.com/blog/2019/06/introduction-powerful-bayes-theorem-data-science/
">introduction-powerful-bayes-theorem-data-science</a>
    5. <a href="https://www.kaggle.com/nicapotato/in-depth-simple-linear-regression
">in-depth-simple-linear-regression</a>
    6. <a href="https://www.analyticsvidhya.com/blog/2020/03/what-is-multicollinearity/
">what-is-multicollinearity</a>
    7. <a href="https://www.analyticsvidhya.com/blog/2020/07/what-is-skewness-statistics/
">what-is-skewness-statistics</a>
    8. <a href="https://www.analyticsvidhya.com/blog/2020/04/statistics-data-science-normal-distribution/
">statistics-data-science-normal-distribution</a>
11. What is the difference between OLS and MLE
12. Bias and Variance
    1. <a href="https://www.analyticsvidhya.com/blog/2020/08/bias-and-variance-tradeoff-machine-learning/
">bias-and-variance-tradeoff-machine-learning</a>
    2. <a href="https://www.analyticsvidhya.com/blog/2020/07/hypothesis-testing-68351/
">hypothesis-testing-68351</a>
13. Naive Bayes
    1. <a href="https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
">naive-bayes-explained</a>
    2. <a href="https://www.saedsayad.com/naive_bayesian.htm
">naive_bayesian</a>
    1. <a href="https://stats.stackexchange.com/questions/256726/linear-regression-what-does-the-f-statistic-r-squared-and-residual-standard-err
">linear-regression-what-does-the-f-statistic-r-squared-and-residual-standard-err</a>
16. Assumptions not me what to do
    1. <a href="https://www.statisticssolutions.com/what-to-do-when-the-assumptions-of-your-analysis-are-violated/
">what-to-do-when-the-assumptions-of-your-analysis-are-violated</a>
17. Interpretation issues in case of transformation
    1. <a href="https://data.library.virginia.edu/interpreting-log-transformations-in-a-linear-model/
">interpreting-log-transformations-in-a-linear-model</a>
    2. <a href="https://towardsdatascience.com/interpreting-the-coefficients-of-linear-regression-cc31d4c6f235
">interpreting-the-coefficients-of-linear-regression-cc31d4c6f235</a>
18. Stepwise forward and backward iterations
    1. <a href="https://stackabuse.com/applying-wrapper-methods-in-python-for-feature-selection/
">applying-wrapper-methods-in-python-for-feature-selection</a>
    2. <a href="https://towardsdatascience.com/stopping-stepwise-why-stepwise-selection-is-bad-and-what-you-should-use-instead-90818b3f52df
">stopping-stepwise-why-stepwise-selection-is-bad-and-what-you-should-use-instead-90818b3f52df</a>
    3. <a href="https://www.kaggle.com/talhahascelik/automated-stepwise-backward-and-forward-selection
">automated-stepwise-backward-and-forward-selection</a>
19. AUC ROC
    1. <a href="https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5
">understanding-auc-roc-curve-68b2303cc9c5</a>
20. Statistics Concepts
    1. <a href="https://www.analyticsvidhya.com/blog/2017/02/basic-probability-data-science-with-examples/
">basic-probability-data-science-with-examples</a>
    2. <a href="https://www.analyticsvidhya.com/blog/2019/09/everything-know-about-p-value-from-scratch-data-science/
">everything-know-about-p-value-from-scratch-data-science</a>
    3. <a href="https://www.analyticsvidhya.com/blog/2017/01/comprehensive-practical-guide-inferential-statistics-data-science/
">comprehensive-practical-guide-inferential-statistics-data-science</a>
    4. <a href="https://analyticsindiamag.com/40-interview-questions-on-statistics-for-data-scientists/
">40-interview-questions-on-statistics-for-data-scientists</a>
    5. <a href="https://towardsdatascience.com/a-very-friendly-introduction-to-confidence-intervals-9add126e714
">a-very-friendly-introduction-to-confidence-intervals-9add126e714</a>
    6. <a href="https://medium.com/acing-ai/what-is-better-a-type-i-or-a-type-ii-error-960f7d1799df
">what-is-better-a-type-i-or-a-type-ii-error-960f7d1799df</a>
    7. <a href="https://www.analyticsvidhya.com/blog/2019/05/statistics-t-test-introduction-r-implementation/
">statistics-t-test-introduction-r-implementation</a>
    8. <a href="https://www.analyticsvidhya.com/blog/2015/09/hypothesis-testing-explained/?utm_source=blog&utm_medium=statistics-t-test-introduction-r-implementation
">LINK</a>
    9. <a href="https://www.analyticsvidhya.com/blog/2019/09/everything-know-about-p-value-from-scratch-data-science/
">everything-know-about-p-value-from-scratch-data-science</a>
21. Logistic regression
    1. <a href="https://medium.com/analytics-vidhya/insiders-view-on-logistic-regression-and-how-do-we-deploy-regression-model-in-gcp-as-batch-c62a64563210
">insiders-view-on-logistic-regression-and-how-do-we-deploy-regression-model-in-gcp-as-batch-c62a64563210</a>
    2. <a href="https://towardsdatascience.com/logistic-regression-b0af09cdb8ad
">logistic-regression-b0af09cdb8ad</a>
    3. <a href="https://www.analyticsvidhya.com/blog/2017/08/skilltest-logistic-regression/
">skilltest-logistic-regression</a>
    4. <a href="https://medium.com/analytics-vidhya/insiders-view-on-logistic-regression-and-how-do-we-deploy-regression-model-in-gcp-as-batch-c62a64563210
">insiders-view-on-logistic-regression-and-how-do-we-deploy-regression-model-in-gcp-as-batch-c62a64563210</a>
22. BoxCox transformation
    1. <a href="https://medium.com/@kangeugine/box-cox-transformation-ba01df7da884
">box-cox-transformation-ba01df7da884</a>
23. Feature Engineering
    1. <a href="https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/
">guide-data-exploration</a>
    2. <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html
">05</a>
    3. <a href="https://www.theanalysisfactor.com/interpreting-interactions-in-regression/
">interpreting-interactions-in-regression</a>
24. Assumptions of Logistic R
25. Trick to enhanc predictive Power models
    1. <a href="https://www.analyticsvidhya.com/blog/2013/10/trick-enhance-power-regression-model-2/
">trick-enhance-power-regression-model-2</a>
26. Probability Calibration
    1. <a href="https://scikit-learn.org/stable/modules/calibration.html
">calibration</a>
    2. <a href="https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration.html
">plot_calibration</a>
    3. <a href="https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/
">calibrated-classification-model-in-scikit-learn</a>
    4. <a href="https://towardsdatascience.com/probability-calibration-for-boosted-trees-24cbd0f0ccae
">probability-calibration-for-boosted-trees-24cbd0f0ccae</a>
27. A B Testing
    1. <a href="https://www.analyticsvidhya.com/blog/2013/06/importance-testing-learning-tests/
">importance-testing-learning-tests</a>
    2. <a href="https://www.analyticsvidhya.com/blog/2013/11/questions-designing-test/
">questions-designing-test</a>
    3. <a href="https://www.analyticsvidhya.com/blog/2016/10/using-pyspark-to-perform-transformations-and-actions-on-rdd/
">using-pyspark-to-perform-transformations-and-actions-on-rdd</a>
28. Pyspark
    1. <a href="https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/
">comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark</a>
    2. <a href="https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/
">spark-dataframe-and-operations</a>
29. PCA and t SNE data reduction
    1. <a href="https://www.analyticsvidhya.com/blog/2017/03/questions-dimensionality-reduction-data-scientist/
">questions-dimensionality-reduction-data-scientist</a>
    2. <a href="https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1
">an-introduction-to-t-sne-with-python-example-5a3a293108d1</a>
    3. <a href="https://www.theanalysisfactor.com/tips-principal-component-analysis/
">tips-principal-component-analysis</a>
    4. <a href="https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c
">a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c</a>
    5. <a href="https://www.analyticsvidhya.com/blog/2019/08/5-applications-singular-value-decomposition-svd-data-science/
">5-applications-singular-value-decomposition-svd-data-science</a>
30. difference between PCA and LDA
    1. <a href="https://sebastianraschka.com/faq/docs/lda-vs-pca.html
">lda-vs-pca</a>
    2. <a href="https://medium.com/@srishtisawla/linear-discriminant-analysis-d38decf48105
">linear-discriminant-analysis-d38decf48105</a>
    3. <a href="https://www.geeksforgeeks.org/ml-linear-discriminant-analysis/
">ml-linear-discriminant-analysis</a>
    4. <a href="https://towardsdatascience.com/is-lda-a-dimensionality-reduction-technique-or-a-classifier-algorithm-eeed4de9953a
">is-lda-a-dimensionality-reduction-technique-or-a-classifier-algorithm-eeed4de9953a</a>
31. Difference between Logistic and LDA
    1. <a href="https://machinelearningmastery.com/linear-discriminant-analysis-for-machine-learning/
">linear-discriminant-analysis-for-machine-learning</a>
    2. <a href="https://datastoriesweb.wordpress.com/2017/06/16/comparing-lda-and-lr/
">comparing-lda-and-lr</a>
    3. <a href="https://stats.stackexchange.com/questions/254124/why-does-logistic-regression-become-unstable-when-classes-are-well-separated/254205
">254205</a>
32. PCA and factor analysis
    1. <a href="https://www.theanalysisfactor.com/the-fundamental-difference-between-principal-component-analysis-and-factor-analysis/
">the-fundamental-difference-between-principal-component-analysis-and-factor-analysis</a>
    2. <a href="https://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi
">what-are-the-differences-between-factor-analysis-and-principal-component-analysi</a>
33. t SNE Data reduction technique
    1. <a href="https://www.datacamp.com/community/tutorials/introduction-t-sne
">introduction-t-sne</a>
34. SVM
    1. <a href="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
">understaing-support-vector-machine-example-code</a>
    2. <a href="https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72
">chapter-2-svm-support-vector-machine-theory-f0812effc72</a>
    3. <a href="https://www.analyticsvidhya.com/blog/2017/10/svm-skilltest/
">svm-skilltest</a>
35. Naive Bayes Algo
    1. <a href="https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c
">naive-bayes-classifier-81d512f50a7c</a>
    2. <a href="https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
">naive-bayes-explained</a>
    3. <a href="https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/
">how-naive-bayes-algorithm-works-with-example-and-full-code</a>
    4. <a href="https://stackoverflow.com/questions/3473612/ways-to-improve-the-accuracy-of-a-naive-bayes-classifier
">ways-to-improve-the-accuracy-of-a-naive-bayes-classifier</a>
 
KEY POINTS
1. The OLS is a distance-minimizing approximation/estimation method, while MLE is a "likelihood" maximization method. Both are used to estimate the parameters of a linear regression model .OLS estimator needs no stochastic assumptions to provide its distance-minimizing solution but need assumptions for justifying that the estimator is BLUE, while MLE starts by assuming a joint probability density/mass function.
2. R-squared: The coefficient of determination, the Sum Squares of Regression divided by Total Sum Squares. In English, translates in the percent of variance explained by the model. The remaining percentage represents the variance explained by error, the E term, that which the model and predictors fail to grasp.
3. Adj. R-squared: Version of the R-Squared that penalizes additional independent variables. Similar to the concept of flexibility in the Bias-Variance tradeoff where high flexibility reduces bias, but puts the model at risk of high variance; the magnitude of effect a single observation can have on the model outcome. This lowers model robustness and model generalization.
4. F-statistic: A measure how significant the fit is. The mean squared error of the model divided by the mean squared error of the residuals. Feeds into the calculation of the P-Value.
5. Prob (F-statistic) or P-Value: The probability that a sample like this would yield the above statistic, and whether the models verdict on the null hypothesis will consistently represent the population. Does not measure effect magnitude, instead measures the integrity and consistency of this test on this group of data.
6. Log-likelihood: The log of the likelihood function.
7. AIC: The Akaike Information Criterion. Adjusts the log-likelihood based on the number of observations and the complexity of the model. Penalizes the model selection metrics when more independent variables are added.
8. BIC: The Bayesian Information Criterion. Similar to the AIC, but has a higher penalty for models with more parameters. Penalizes the model selection metrics when more independent variables are added.
9. In a regression (or ANOVA), we build a model based on a sample dataset which enables us to predict outcomes from a population of interest. To do so, the following three components are calculated in a simple linear regression from which the other components can be calculated, e.g. the mean squares, the F-value, the R2 (also the adjusted R2), and the residual standard error (RSE): total sums of squares (SStotal) residual sums of squares (SSresidual) model sums of squares (SSmodel) Each of them are assessing how well the model describes the data and are the sum of the squared distances from the data points to fitted model (illustrated as red lines in the plot below).
10. The SStotal assess how well the mean fits the data. Why the mean? Because the mean is the simplest model we can fit and hence serves as the model to which the least-squares regression line is compared to. This plot using the cars dataset illustrates that:
11. http://analyticspro.org/2016/03/15/r-tutorial-how-to-interpret-f-statistic-in-regression-models/
12. How is F Statistic different from R Squared ?
13. R squared provides a measure of strength of relationship between our predictors and our response variable and it does not comment on whether the relationship is statistically significant. F Statistic gives us a power to judge whether that relationship is statistically significant in other words it comments on whether or R² is significant or not.
14. What should i do with F statistic in Regression model ?
15. If my F-statistic is significant that gives me extra confidence on the R² value that i have got .
16. In case i get insignificant F-Statistic or if p values for F are greater that level of significance ( say 0.05 or 0.01 ) then personally i would stay away from that model since i will not be able to confidently comment on the R² values
17. binary logistic regression requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal. observations should be independent of each other little or no multi-collinearity among the independent variables independent variables are linearly related to the log odds
18. http://theprofessionalspoint.blogspot.com/2019/03/advantages-and-disadvantages-of_4.html
19. Space and time complexity : For example, the standard support vector machine (SVM) algorithm has a training time complexity of O(m3) and a space complexity of O(m2) [29], where m is the number of
20. training samples. Therefore, an increase in the size m will drastically affect the time and memory needed to train the SVM algorithm and may even become computationally
21. infeasible on very large datasets. Many other ML algorithms also exhibit high time complexity: for example, the time complexity of principal component analysis is O(mn2+n3), that of logistic regression O(mn2+n3), that of locally weighted linear regression O(mn2+n3), and that of Gaussian discriminative analysis O(mn2+n3) [30], where m is the number of samples and n the number of features. Hence, for all these algorithms, the time needed to perform the computations will increase exponentially with increasing data size and may even render the algorithms unusable for very large datasets.
