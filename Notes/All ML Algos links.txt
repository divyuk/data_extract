## Encoding techniques

https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/

## difference between normalization and standardization
https://www.geeksforgeeks.org/normalization-vs-standardization/
https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/


#######Boosting ###########

https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/
https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/
https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/


##GBM Internal working####
https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d
https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab


###XGboost and Light GBM difference ###########

https://neptune.ai/blog/xgboost-vs-lightgbm


###stacking and blending difference 

https://neptune.ai/blog/ensemble-learning-guide


########Analytics vidya links#####
https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/
https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/


###########pyspark############
https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/?utm_source=blog&utm_medium=streaming-data-pyspark-machine-learning-model
https://www.analyticsvidhya.com/blog/2019/12/streaming-data-pyspark-machine-learning-model/


## Linear Regression

https://www.analyticsvidhya.com/blog/2017/07/30-questions-to-test-a-data-scientist-on-linear-regression/
https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/

#####Statistics
https://www.analyticsvidhya.com/blog/2017/01/comprehensive-practical-guide-inferential-statistics-data-science/
https://www.analyticsvidhya.com/blog/2020/07/hypothesis-testing-68351/
https://www.analyticsvidhya.com/blog/2020/06/statistics-analytics-hypothesis-testing-z-test-t-test/
https://www.analyticsvidhya.com/blog/2019/06/introduction-powerful-bayes-theorem-data-science/
https://www.kaggle.com/nicapotato/in-depth-simple-linear-regression
https://www.analyticsvidhya.com/blog/2020/03/what-is-multicollinearity/
https://www.analyticsvidhya.com/blog/2020/07/what-is-skewness-statistics/
https://www.analyticsvidhya.com/blog/2020/04/statistics-data-science-normal-distribution/

###########What-is-the-difference-between-OLS-and-MLE##############
The OLS is a distance-minimizing approximation/estimation method, while MLE is a "likelihood" maximization method. Both are used to estimate the parameters of a linear regression model .OLS estimator needs no stochastic assumptions to provide its distance-minimizing solution but need assumptions for justifying that the estimator is BLUE, while MLE starts by assuming a joint probability density/mass function.


### Bias and Variance 
https://www.analyticsvidhya.com/blog/2020/08/bias-and-variance-tradeoff-machine-learning/
https://www.analyticsvidhya.com/blog/2020/07/hypothesis-testing-68351/


###############Naive Bayes
https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
https://www.saedsayad.com/naive_bayesian.htm

R-squared: The coefficient of determination, the Sum Squares of Regression divided by Total Sum Squares. In English, translates in the percent of variance explained by the model. The remaining percentage represents the variance explained by error, the E term, that which the model and predictors fail to grasp.
Adj. R-squared: Version of the R-Squared that penalizes additional independent variables. Similar to the concept of flexibility in the Bias-Variance tradeoff where high flexibility reduces bias, but puts the model at risk of high variance; the magnitude of effect a single observation can have on the model outcome. This lowers model robustness and model generalization.
F-statistic: A measure how significant the fit is. The mean squared error of the model divided by the mean squared error of the residuals. Feeds into the calculation of the P-Value.

Prob (F-statistic) or P-Value: The probability that a sample like this would yield the above statistic, and whether the models verdict on the null hypothesis will consistently represent the population. Does not measure effect magnitude, instead measures the integrity and consistency of this test on this group of data.

Log-likelihood: The log of the likelihood function.
AIC: The Akaike Information Criterion. Adjusts the log-likelihood based on the number of observations and the complexity of the model. Penalizes the model selection metrics when more independent variables are added.
BIC: The Bayesian Information Criterion. Similar to the AIC, but has a higher penalty for models with more parameters. Penalizes the model selection metrics when more independent variables are added.

############################

https://stats.stackexchange.com/questions/256726/linear-regression-what-does-the-f-statistic-r-squared-and-residual-standard-err


In a regression (or ANOVA), we build a model based on a sample dataset which enables us to predict outcomes from a population of interest. To do so, the following three components are calculated in a simple linear regression from which the other components can be calculated, e.g. the mean squares, the F-value, the R2 (also the adjusted R2), and the residual standard error (RSE): total sums of squares (SStotal) residual sums of squares (SSresidual) model sums of squares (SSmodel) Each of them are assessing how well the model describes the data and are the sum of the squared distances from the data points to fitted model (illustrated as red lines in the plot below).

The SStotal assess how well the mean fits the data. Why the mean? Because the mean is the simplest model we can fit and hence serves as the model to which the least-squares regression line is compared to. This plot using the cars dataset illustrates that:

##########################
http://analyticspro.org/2016/03/15/r-tutorial-how-to-interpret-f-statistic-in-regression-models/

How is F Statistic different from R Squared ?
R squared provides a measure of strength of relationship between our predictors and our response variable and it does not comment on whether the relationship is statistically significant. F Statistic gives us a power to judge whether that relationship is statistically significant in other words it comments on whether or R² is significant or not.

What should i do with F statistic in Regression model ?
If my F-statistic is significant that gives me extra confidence on the R² value that i have got .
In case i get insignificant F-Statistic or if p values for F are greater that level of significance ( say 0.05 or 0.01 ) then personally i would stay away from that model since i will not be able to confidently comment on the R² values


########################Assumptions not me what to do##############

https://www.statisticssolutions.com/what-to-do-when-the-assumptions-of-your-analysis-are-violated/


######### Interpretation issues in case of transformation###############
https://data.library.virginia.edu/interpreting-log-transformations-in-a-linear-model/
https://towardsdatascience.com/interpreting-the-coefficients-of-linear-regression-cc31d4c6f235



##############Stepwise forward and backward iterations##########
https://stackabuse.com/applying-wrapper-methods-in-python-for-feature-selection/
https://towardsdatascience.com/stopping-stepwise-why-stepwise-selection-is-bad-and-what-you-should-use-instead-90818b3f52df
https://www.kaggle.com/talhahascelik/automated-stepwise-backward-and-forward-selection


###########AUC ROC
https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5






###########Statistics Concepts###############
https://www.analyticsvidhya.com/blog/2017/02/basic-probability-data-science-with-examples/
https://www.analyticsvidhya.com/blog/2019/09/everything-know-about-p-value-from-scratch-data-science/
https://www.analyticsvidhya.com/blog/2017/01/comprehensive-practical-guide-inferential-statistics-data-science/
https://analyticsindiamag.com/40-interview-questions-on-statistics-for-data-scientists/
https://towardsdatascience.com/a-very-friendly-introduction-to-confidence-intervals-9add126e714
https://medium.com/acing-ai/what-is-better-a-type-i-or-a-type-ii-error-960f7d1799df
https://www.analyticsvidhya.com/blog/2019/05/statistics-t-test-introduction-r-implementation/
https://www.analyticsvidhya.com/blog/2015/09/hypothesis-testing-explained/?utm_source=blog&utm_medium=statistics-t-test-introduction-r-implementation
https://www.analyticsvidhya.com/blog/2019/09/everything-know-about-p-value-from-scratch-data-science/


###Logistic regression###############

https://medium.com/analytics-vidhya/insiders-view-on-logistic-regression-and-how-do-we-deploy-regression-model-in-gcp-as-batch-c62a64563210
https://towardsdatascience.com/logistic-regression-b0af09cdb8ad
https://www.analyticsvidhya.com/blog/2017/08/skilltest-logistic-regression/
https://medium.com/analytics-vidhya/insiders-view-on-logistic-regression-and-how-do-we-deploy-regression-model-in-gcp-as-batch-c62a64563210


################BoxCox transformation#############
https://medium.com/@kangeugine/box-cox-transformation-ba01df7da884



########Feature Engineering
https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/
https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html
https://www.theanalysisfactor.com/interpreting-interactions-in-regression/






#############Assumptions of Logistic R ################

binary logistic regression requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal. observations should be independent of each other little or no multi-collinearity among the independent variables independent variables are linearly related to the log odds

### Trick to enhanc predictive Power models###

https://www.analyticsvidhya.com/blog/2013/10/trick-enhance-power-regression-model-2/


## Probability Calibration#################
https://scikit-learn.org/stable/modules/calibration.html
https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration.html
https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/
https://towardsdatascience.com/probability-calibration-for-boosted-trees-24cbd0f0ccae


### A/B Testing
https://www.analyticsvidhya.com/blog/2013/06/importance-testing-learning-tests/
https://www.analyticsvidhya.com/blog/2013/11/questions-designing-test/
https://www.analyticsvidhya.com/blog/2016/10/using-pyspark-to-perform-transformations-and-actions-on-rdd/

########Pyspark

https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/
https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/


####PCA and t-SNE data reduction
https://www.analyticsvidhya.com/blog/2017/03/questions-dimensionality-reduction-data-scientist/
https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1
http://theprofessionalspoint.blogspot.com/2019/03/advantages-and-disadvantages-of_4.html
https://www.theanalysisfactor.com/tips-principal-component-analysis/
https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c

https://www.analyticsvidhya.com/blog/2019/08/5-applications-singular-value-decomposition-svd-data-science/

## difference between PCA and LDA

https://sebastianraschka.com/faq/docs/lda-vs-pca.html
https://medium.com/@srishtisawla/linear-discriminant-analysis-d38decf48105
https://www.geeksforgeeks.org/ml-linear-discriminant-analysis/
https://towardsdatascience.com/is-lda-a-dimensionality-reduction-technique-or-a-classifier-algorithm-eeed4de9953a

### Difference between Logistic and LDA
https://machinelearningmastery.com/linear-discriminant-analysis-for-machine-learning/
https://datastoriesweb.wordpress.com/2017/06/16/comparing-lda-and-lr/
https://stats.stackexchange.com/questions/254124/why-does-logistic-regression-become-unstable-when-classes-are-well-separated/254205

## PCA and factor analysis
https://www.theanalysisfactor.com/the-fundamental-difference-between-principal-component-analysis-and-factor-analysis/
https://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi

###t-SNE Data reduction technique

https://www.datacamp.com/community/tutorials/introduction-t-sne

#######SVM#################


https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72
https://www.analyticsvidhya.com/blog/2017/10/svm-skilltest/

### Naive Bayes Algo
 
https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c
https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/

https://stackoverflow.com/questions/3473612/ways-to-improve-the-accuracy-of-a-naive-bayes-classifier






Space and time complexity : For example, the standard support vector machine (SVM) algorithm has a training time complexity of O(m3) and a space complexity of O(m2) [29], where m is the number of
training samples. Therefore, an increase in the size m will drastically affect the time and memory needed to train the SVM algorithm and may even become computationally
infeasible on very large datasets. Many other ML algorithms also exhibit high time complexity: for example, the time complexity of principal component analysis is O(mn2+n3), that of logistic regression O(mn2+n3), that of locally weighted linear regression O(mn2+n3), and that of Gaussian discriminative analysis O(mn2+n3) [30], where m is the number of samples and n the number of features. Hence, for all these algorithms, the time needed to perform the computations will increase exponentially with increasing data size and may even render the algorithms unusable for very large datasets. 